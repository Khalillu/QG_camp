## RNN（循环神经网络）：

### **结构：**

输入是x，经过变换$y=f(Wx+b)$，得到输出y

![img](https://i-blog.csdnimg.cn/blog_migrate/2907c24280f0e922bb818c4fcc21673f.png)

为了建模序列问题，RNN引入了隐状态h(hidden state)的概念，**隐状态h可以对序列性的数据提取特征，接着再转换为输出。**

### **计算过程：**

![img](https://i-blog.csdnimg.cn/blog_migrate/ea55cac3d54f9e473ffc12c5302c2667.png)

图示中记号的含义是：

- a）圆圈或方块表示的是向量。
- b）**一个箭头**就表示对该向量做一次**变换**。如上图中![h_{0}](https://i-blog.csdnimg.cn/blog_migrate/c978634fafcd02242574b3ab4d80177f.gif)和![x_{1}](https://i-blog.csdnimg.cn/blog_migrate/d653c6079e1c1dc80300082552738a18.gif)分别有一个箭头连接，就表示对$h_0$**和$x_1$​各做了一次变换**

即h1基于上一个隐藏层的状态$h_0$和当前的输入$x_1$计算得来

泛化到任意时刻，$h_t=f(Wh_{t-1} + Ux_t)$​

此时的f一般是**tanh、sigmoid、ReLU**等非线性激活函数

且在实践中$h_t$​​一般只包含前面若干步，而非之前所有步的隐藏状态

每一个箭头就表示对对应的向量做一次类似于$f(Wx+b)$的变换，对h1进行一次变换，得到输出y1

![img](https://i-blog.csdnimg.cn/blog_migrate/3c596042ae052beef720f35f591ca074.png)

在这个循环的结构中，每个神经网络的模块A，读取某个输入$x_t$，并输出一个值$h_t$（此处$h_t$即为表示上图中的$y_t$），然后不断循环

![img](https://i-blog.csdnimg.cn/blog_migrate/fc8404d386323a3e8848f3577c942a5a.png)

将上述神经网络结构展开之后：

![img](https://i-blog.csdnimg.cn/blog_migrate/53366d2d0ca657892178eb3abab32f92.png)

即**每个神经网络模块会把消息传递给下一个。**

揭示了RNN本质上是与**序列和列表相关**的

### **特点：**

1. 每一步使用的**参数**U、W、b都是**一样**的，也就是说**每个**步骤的参数都是**共享**的，这是RNN的重要特点
2. RNN的权值是在**同一个向量**中，只是不同时刻而已
3. 输入序列和输出序列必须要是等长的



### **定义：**

RNN可以处理**时间序列数据或具有时序特征**的数据。它的关键特点是能够**通过**隐藏层状态的**循环连接**，使得信息能够**跨时间步**进行传递。

### **工作原理：**

RNN的每个时刻不仅**接受当前输入的数据**，还**接收前一个时刻隐藏层的输出**，从而使模型能够“记住”之前的输入信息。

### **优点：**

RNN可以处理**变长的序列数据**，广泛应用于**时间序列预测、语音识别、自然语言处理**等任务。

![image-20250407210226250](https://khalillu.oss-cn-guangzhou.aliyuncs.com/khalillu/20250407210226344.png)

激活函数Tanh：作用在于帮助调节流经网络的值，使得数值始终限制在-1到1之间

### **局限性：**

长期依赖（Long-TermDependencies）问题：

eg：

![img](https://i-blog.csdnimg.cn/blog_migrate/4e618cf98f970f5ea2a9d9694ecea991.png)

“如果我们试着预测这句话中“the clouds are in the sky”最后的这个词“sky”，我们并不再需要其他的信息，因为**很显然下一个词**应该是sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是**非常小**的，RNN**可以**学会使用先前的信息。”

![img](https://i-blog.csdnimg.cn/blog_migrate/3a31e94666212e1c7e120ac55474680e.png)

“比如我们试着去预测“I grew up in France...I speak fluent French”最后的词“French”。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的“France”的上下文。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。”

***当间隔不断增大时，RNN会丧失学习到连接如此远的信息的能力***

***RNN会受到短时记忆的影响。如果一条序列足够长，那他们将很难将信息从较早的时间步传送到后面的时间步***

**RNN会面临梯度消失问题**

在递归神经网络中，获得小梯度更新的层会停止学习——那些通常是较早的层。由于这些层**不学习**，RNN会**忘记它在较长序列中以前看到的内容**，因此RNN**只具有短时记忆**。

### 衡量语言模型好坏的指标：

**困惑度（perplexity）**

**$\pi=\frac{1}{2}\sum_{t=1}^{n}-\log{p(x_t|x_{t-1},...)}$​**

p是语言模型的预测概率，$x_t$是真实词

* 历史原因NLP使用困惑度exp($\pi$)来衡量，是平均每次可能选项

* 1表示完美，无穷大是最差情况

### 梯度裁剪：

* 迭代中计算这T个时间步上的梯度，在方向传播过程中产生长度为O(T)的矩阵乘法链，导致数值不稳定

* 梯度裁剪能有效预防梯度爆炸

  * 如果梯度长度超过$\theta$，那么拖影回长度$\theta$

  $g\leftarrow{min(1,\frac{\theta}{||g||}})g$

  即若值处于正常范围内，那就不需要处理g的大小；

  若超出范围，就将g映射到一个正常范围内。

## ***LSTM***（长短期记忆网络）：

LSTM的“记忆”我们叫做细胞/cells，你可以直接把它们想做黑盒，这个黑盒的**输入为前状态![h_{t-1}](https://latex.csdn.net/eq?h_%7Bt-1%7D)和当前输入![x_{t}](https://latex.csdn.net/eq?x_%7Bt%7D)**。这些“细胞”会决定哪些之前的信息和状态需要保留/记住，而哪些要被抹去。实际的应用中发现，这种方式可以有效地保存很长时间之前的关联信息。

#### 定义：

LSTM是一种特殊的RNN结构，它旨在解决RNN在处理长期依赖时面临的**梯度消失和梯度爆炸**问题。通过**引门控**机制来控制信息的流动

#### 结构：

LSTM的核心是由三个门（**输入门、遗忘门和输出门**）组成的。每个门通过控制**信息的流动**，**决定**哪些信息应该**记住**，哪些信息应该**忘记**，哪些信息应该**输出**。

* 忘记门：将值朝0减少
* 输入门：决定是不是忽略掉输入数据
* 决定是不是使用隐状态

LSTM中的重复模块则包含四个交互的层，三个Sigmoid和一个tanh层，并以一种非常特殊的方式进行交互

$\sigma$表示Sigmoid激活函数，把值压缩在0~1之间，而不是-1~1之间。这样的设置有助于更新或忘记信息：

1. 任何数乘以0都等于0，这部分信息将会剔除掉
2. 任何数乘以1都得到它本身，则说明该部分信息被完美地保存下来了

1则记住，0则忘掉

#### 原则：

因为记忆能力有限，记住重要的，忘记无关紧要的

#### 优点：

LSTM能够有效**地捕捉长期依赖关系**，克服了传统RNN在长序列训练中遇到的困难。

![image-20250407210627537](https://khalillu.oss-cn-guangzhou.aliyuncs.com/khalillu/20250407210627627.png)

#### 三门介绍：

##### 遗忘门：

LSTM第一步决定我们会从细胞状态中丢弃什么信息。

遗忘门会读取上一个输出$h_{t-1}$和当前输入$x_t$，做一个Sigmoid的非线性映射，然后输出一个向量$f_t$（由于Sigmoid激活函数，该向量每一个维度的值都在**0到1**之间），最后与**细胞状态**$C_{t-1}$相乘

eg：

类比到语言模型的例子中，则是基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到**新的主语**，我们希望**忘记旧的主语**，进而决定丢弃信息。

![img](https://i-blog.csdnimg.cn/blog_migrate/2e697db05c0f6ba33a33b6933f8b1a18.png)

对于上图右侧公式中的权值$W_f$，准确地说其实是不共享的，即是不一样的。

$f_t=\sigma(W_f[h_{t-1},X_t] + b_f)$

展开后

$f_t=\sigma(W_{f_h}h_{t-1} + W_{f_x}x_t + b_f)$

![img](https://i-blog.csdnimg.cn/blog_migrate/dadaebee487d7868ff40a0f4f7900d05.gif)

##### 输入门：

LSTM下一步是确定什么样的新信息被存放在细胞状态中：

1. Sigmoid层称“输入门层”决定什么值我们将要更新
2. 一个tanh层创建一个新的候选值向量${C_t}$，会被加入到状态中。

eg：

在我们语言模型的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语，进而确定更新的信息。

![img](https://i-blog.csdnimg.cn/blog_migrate/399749776a03b6551922b9d1738e3dfe.png)

$i_t=\sigma(W_i\cdot[h_{t-1},x_t] + b_i)$

$C_t=tanh(W_C\cdot[h_{t-1},x_t] + b_C)$

展开后：

$i_t=\sigma(W_{ih}h_{t-1} + W_{ix}x_t + b_i)$

$C_t=tanh(W_{Ch}h_{t-1} + W_{Cx}x_t + b_C)$

![img](https://i-blog.csdnimg.cn/blog_migrate/f46a5e5252f9648a63595b7229559d50.gif)

此时把**旧状态与f_t相乘**，丢弃掉我们确定需要丢弃的信息，**接着加上$i_t*C_t$​**。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。

![img](https://i-blog.csdnimg.cn/blog_migrate/d4be9a204472f0fd5fe762a2f40a0541.png)

![img](https://i-blog.csdnimg.cn/blog_migrate/5cf4eae0299366eb2ebd7a213c7faa04.gif)

eg：

在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方，类似更新细胞状态。

#### 输出门：

LSTM最后一步需要确定输出什么值，该输出会基于我们的细胞状态。

首先运行一个**Sigmoid层**来确定细胞状态的**哪个部分将输出出去**。接着，把细胞状态通过**tanh**处理（得到一个-1~1的值）并将它和**Sigmoid门的输出相乘**，最终我们仅仅会输出我们确定输出的那部分。

![img](https://i-blog.csdnimg.cn/blog_migrate/8289b41615409092666db6ffdf594ffc.png)

$o_t=\sigma(W_o[h_{t-1},x_t] + b_o)$

$h_t = o_t * tanh(C_t)$

展开后：

$o_t = \sigma(W_{oh}h_{t-1} + W_{ox}x_t + b_0)$

![img](https://i-blog.csdnimg.cn/blog_migrate/9aef6d802c63d9539b94c1771f85bceb.gif)
