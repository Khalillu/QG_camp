## 爬虫

### 基本结构：

#### 定义：

爬虫是一个获取正常浏览器可以获取的数据的自动化获取程序。

要干的事情：

1. 找到我们需要的网页，然后把它们一个一个处理一遍。

#### 问题：

怎么找到我们需要的网页的入口？

答：入口的来源有两种，一种是已知地址，比如我们院的网站的教学研究人员：教学研究人员-WISE；另外一种是你可以通过前面爬下来的网页中获取入口，比如这个网页上所有老师的个人主页。这一点很重要，后面我们在把小爬虫逐步变大的时候还要反复回到这里。

怎么处理我们需要处理的网页？

答：让我们回忆一下我们使用浏览器的过程：你先把一个地址复制进浏览器的框框里面，或者点开一个链接；然后浏览器的进度条跑一下（有可能快，也有可能慢），然后我们就在浏览器里面看到了数据。首先，进度条跑得过程完成了一个对网页的请求，然后浏览器把请求下来的数据进行处理，然后输出出来。这是一个极其简化但是不太准确的对于浏览器工作原理的描述。那么爬虫和浏览器有什么不同呢？一般来说，我们是只需要解析网页，而不需要渲染浏览器环境的；另外，我们需要特定网页的特点数据，因此要用一定的方式把数据组织并储存起来。所以，爬虫的核心模块有三个：**请求、解析、储存。**

### 基本案例：

```python
import requests
from bs4 import BeautifulSoup

# 调用get函数，将网页请求下来
r = requests.get('http://www.wise.xmu.edu.cn/people/faculty')
# r是一个包含了整个HTTP协议所需要的各种各样的东西的对象。
html = r.content
# 查看源文件，查看标签

# 创建一个BeautifulSoup对象
soup = BeautifulSoup(html, 'html.parser')
# html.parser是解析器

# 首先提取这部分代码的第一行，定位到这部分代码
# 使用BeaurifulSoup对象的find方法，这个方法的意思是找到带有‘div’这个标签
# 并且参数包含“class='people_list'”的HTML代码，如果有多个的话，find方法就取第一个。
# 现在，我们要取出所有“a”标签里面的内容
div_people_list = soup.find('div', attrs={'class':'people_list'})

# 使用find_all方法取出所有标签为“a”并且参数包含“target = '_blank'”的代码，返回一个列表。
# "a"标签里面的"href"参数是我们需要的老师个人主页的信息，而标签里面的文字是老师的名字。
# attrs参数包含{...}
a_s = div_people_list.find_all('a', attrs={'target':'_blank'})

# 把a标签里面的"href"参数的值提取出来，赋值给url，使用get_text()提取文字
for a in a_s:
    url = a['href']
    name = a.get_text()
    print(name,url)
```

### 请求：

HTTP协议的传输主要通过HTTP报文实现的：

1. 客户端（比如浏览器） ：发送请求报文到服务器
2. 服务器：接收请求报文并处理
3. 服务器：发送响应报文给客户端
4. 客户端：处理接收到的内容。

客户端请求的方法也是不一样的：

一种是客户端不发送数据，一种是客户端发送数据，然后接收响应报文。前者是get方法，后者是post方法。

HTTP协议是一种在互联网上传输文件的协议，主要过程是客户端发送请求报文、服务器接收并发送响应报文、客户端接收；访问某个服务器资源需要知道它的URL；主要的HTTP请求方法有get（客户端不发数据）和post（客户端发数据）

```python
import requests

r1 = requests.get("https://cn.bing.com/search?q=requests")

post_data ={
    'stock':'000001',
    'searchkey':'',
    'category':'category_ndbg_szsh',
    'pageNum':'1',
    'pageSize':'',
    'column':'szse_main',
    'tabName':'fulltext',
    'sortName':'',
    'limit':'',
    'seDate':''
}

r2 = requests.post('https://www.cninfo.com.cn/cninfo-new/announcement/query',data=post_data)

# get方法和post方法的使用如上。这里的返回值是一个对象，这个对象包括了各种各样的属性和方法

print(r1.status_code)
print(r1.encoding)
print(r1.content)

print(r1.json) # 把请求回来的json数据转成python字典并返回

r3 = requests.get('http://www.cninfo.com.cn/finalpage/2015-03-13/1200694563.PDF'
                  ,stream=True)   # 请求
r3.raw.read(10)

def getHTML(url):
    r = requests.get(url)
    return r.content

if __name__ == '__main__':
    url = 'https://zhuanlan.zhihu.com/xmucpp'
    html = getHTML(url)
    print(html)
```

### 解析

#### bs4解析HTML

在审查元素里面去找我们需要的数据，加下来要做的事是用bs4这个强大的工具，通过前面提到的标签和标签的属性定位到某个标签。

找合适的标签最简单的方法是一个一个标签定位下去

直接索引 or find方法

find方法里面也可以加上attrs参数，然后用字典传入我们想找的标签属性。

若我们想去出某一类标签下的所有HTML，那就用find_all方法。若想要某个标签属性里面的值，那么使用字典的语法索引就可以。若想要标签里面的文字，则使用get_text()方法

```python
import requests
from bs4 import BeautifulSoup

# 在审查元素里面去找我们需要的数据，接下来要做的事情是，
# 用bs4这个强大的工具，通过前面提到的标签和标签的属性定位到某个标签。

def getHTML(url):
    r = requests.get(url)
    return r.content
# parser是解析器。
def parseHTML(html):
    soup = BeautifulSoup(html,'html.parser')
    # HTML的标签习惯用“-”连接，比如“list-ct”，而不是“_”，比如
    body=soup.body
    company_middle = body.find('div',
                               attrs={'class':'middle'})
    company_list_ct = company_middle.find('div',
                                attrs={'class':'list-ct'})

    for company_ul in company_list_ct.find_all('ul'
            ,attrs = {'class':'company-list'}):
        for company_li in company_ul.find_all('li'):
            company_url = company_li.a['href']
            company_info = company_li.get_text()
            print(company_info,company_url)
if __name__ == '__main__':
    URL = 'https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id9'
    html = getHTML(URL)
    parseHTML(html)
```

### 储存

一般来说要根据后期管理和分析的需要来选择储存方式。总体来说可以分为文件和数据库。一般使用文件csv格式保存。

```python
import pandas as pd
import csv
import codecs
# -*- coding: utf-8 -*-
# python列表or元组与csv的转换

# csv.reader(file)    # 读出csv文件
# csv.writer(file)    # 写入csv文件
# writer.writerow(data)   # 写入一行数据
# writer.writerows(data)  # 写入多行数据
#
# # Python字典与csv的转换
# csv.DictReader(file) # 读出csv文件
#
# csv.DictWriter(file) # 写入csv文件
# writer.writeheader()    # 写文件头
# writer.writerow(data)   # 写入一行数据
# writer.writerows(data)  # 写入多行数据

import requests
from bs4 import BeautifulSoup

# 在审查元素里面去找我们需要的数据，接下来要做的事情是，
# 用bs4这个强大的工具，通过前面提到的标签和标签的属性定位到某个标签。

def getHTML(url):
    r = requests.get(url)
    return r.content
# parser是解析器。
def parseHTML(html):
    soup = BeautifulSoup(html,'html.parser')
    # HTML的标签习惯用“-”连接，比如“list-ct”，而不是“_”，比如
    body=soup.body
    company_middle = body.find('div',
                               attrs={'class':'middle'})
    company_list_ct = company_middle.find('div',
                                attrs={'class':'list-ct'})
    company_list = []

    for company_ul in company_list_ct.find_all('ul'
            ,attrs = {'class':'company-list'}):
        for company_li in company_ul.find_all('li'):
            company_url = company_li.a['href']
            company_info = company_li.get_text()
            company_list.append([company_info.encode('utf-8'),company_url.encode('utf-8')])

    return company_list

def writeCSV(file_name,data_list):
    with codecs.open(file_name,'wb') as f:
        writer = csv.writer(f)
        for data in data_list:
            writer.writerow(data)

if __name__ == '__main__':
    URL = 'https://cycling.today/2025-tour-de-france-live-stream/'
    html = getHTML(URL)
    data_list = parseHTML(html)
    writeCSV('test.csv',data_list)
```

实战例子：

```python
# 终于自己做出来了 T T
from DrissionPage import ChromiumPage
import pandas as pd
from tqdm import tqdm
import time

page = ChromiumPage()
page.get('http://www.tqyb.com.cn/')


def get_info():
    # 页面滚动到底部，方便查看爬到第几页
    time.sleep(2)
    # 定位包含天气信息的ul


    # 提取温度、雨量、风速信息

    update_time = page.ele('#wind-date').text.replace('数据更新时间：', '')

    temp_data = []
    rain_data = []
    wind_data = []

    tbody = page.ele('#statistics-temp')

    for tr in tbody.eles('tag:tr')[1:]:
        cells = tr.eles('tag:td')
        if len(cells) >= 2:  # 确保有区域名和风速数据
            area = cells[0].text  # 区域名称
            temp1 = cells[1].text  # 温度值
            temp2 = cells[2].text
            temp3 = cells[3].text
            temp4 = cells[4].text
            temp5 = cells[5].text
            temp_data.append({
                '区域': area,
                '温度1': temp1,
                '温度2': temp2,
                '温度3': temp3,
                '温度4': temp4,
                '温度5': temp5,
                '更新时间': update_time
            })

    tbody = page.ele('#statistics-rain')

    # 提取温度、雨量、风速信息

    for tr in tbody.eles('tag:tr')[1:]:
        cells = tr.eles('tag:td')
        if len(cells) >= 2:  # 确保有区域名和风速数据
            area = cells[0].text  # 区域名称
            rain1 = cells[1].text  # 雨量值
            rain2 = cells[2].text
            rain3 = cells[3].text
            rain4 = cells[4].text
            rain5 = cells[5].text
            rain_data.append({
                '区域': area,
                '雨量1': rain1,
                '雨量2': rain2,
                '雨量3': rain3,
                '雨量4': rain4,
                '雨量5': rain5,
                '更新时间': update_time
            })

    tbody = page.ele('#statistics-wind')

    for tr in tbody.eles('tag:tr')[1:]:
        cells = tr.eles('tag:td')
        if len(cells) >= 2:  # 确保有区域名和风速数据
            area = cells[0].text  # 区域名称
            wind_speed1 = cells[1].text  # 风速值
            wind_speed2 = cells[2].text
            wind_speed3 = cells[3].text
            wind_speed4 = cells[4].text
            wind_speed5 = cells[5].text
            wind_data.append({
                '区域': area,
                '风速1': wind_speed1,
                '风速2': wind_speed2,
                '风速3': wind_speed3,
                '风速4': wind_speed4,
                '风速5': wind_speed5,
                '更新时间': update_time
            })

    return [[x, y, z] for x, y, z in zip(temp_data, rain_data, wind_data)]

data = get_info()

if data:
    df = pd.DataFrame(data)
    df.to_csv('TempRainWind_data.csv', index=False)
    print(f"成功保存{len(data)}条温度、雨量和风速数据")
    for item in data:
        print(f"0~5℃温度: {item[0]['温度1']}, 5~35℃温度：{item[0]['温度2']},"
              f" 35~37℃温度：{item[0]['温度3']}, 37~39℃温度：{item[0]['温度4']},39℃~：{item[0]['温度5']},"
              f"雨量: 降水量0~10mm：{item[1]['雨量1']},雨量: 降水量10~20mm：{item[1]['雨量2']},"
              f"雨量: 降水量20~30mm：{item[1]['雨量3']},雨量: 降水量30~40mm：{item[1]['雨量4']},雨量: 降水量40~mm：{item[1]['雨量5']},"
              f"风速: 0~10.8m/s：{item[2]['风速1']},风速: 10.8~17.2m/s：{item[2]['风速2']},风速: 17.2~24.5m/s：{item[2]['风速3']},风速: 24.5~32.7m/s：{item[2]['风速4']},风速: 32.7~m/s：{item[2]['风速5']}")
else:
    print("未获取到数据")

# 关闭浏览器
page.close()
```