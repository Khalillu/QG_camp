## GNN（图神经网络）

### 背景：

现有的深度学习算法为什么不能用在图中的原因：

由于图的复杂性使得现有的深度学习算法在处理时面临着巨大的挑战，因为图是**不规则**的，每个图都有一个**大小可变的无序节点**，每个节点都有**不同数量的相邻节点**，导致一些重要的操作（如卷积）在图像上很容易计算，但**不**再适合直接**作用于图**。此外，现有深度学习算法的一个核心假设是数据样本之间**彼此独立**。然而对于图而言，情况并非这样，因为图中的每个数据样本（节点）都会有边与图中其他实数据样本（节点）**相关**，这些信息可用于捕获实例之间的**相互依赖关系**。

### 定义：

用于处理图数据的神经网络结构

###  图嵌入(Graph Embedding)与GNN的关系：

图嵌入的深度学习方法也属于图神经网络，包括基于图自动编码器的算法（DNGR和SDNE）和无监督训练的图卷积神经网络（如GraphSage）

![img](https://pica.zhimg.com/v2-9308db5110b806f4334e910a418fe908_r.jpg)

### 类别：

#### 图卷积网络（Graph Convolution Networks）：

##### 定义：

图卷积网络将卷积运算从传统数据（例如图像）推广到图数据。

##### 核心思想：

学习一个函数$f(.)$，通过该映射图中的节点$v_i$可以聚合它自己的特征$x_i$与它的邻居特征$x_j$（$ j \in N(v_i)$）来生成节点$v_i$的新表示。图卷积网络是许多复杂图神经网络模型的基础，包括基于自动编码器的模型、生成模型和时空网络等。下图直观地展示了图神经网络学习节点表示的步骤。

![img](https://pic4.zhimg.com/v2-5bbdac5ae4b66bec5dc61593a2a5d29d_r.jpg)

##### GCN类别：

GCN又可以分为两大类，基于谱（**spectral-based**）和基于空间（**spatial-based**）。基于谱的方法从**图信号处理**的角度引入滤波器来定义图卷积，其中**图卷积**被解释为从图信号中**去除噪声**。基于空间的方法将图卷积表示为从**论语**聚合特征信息，当图卷积网络的算法在节点层次运行时，图**池化模块**可以与**图卷积层交错**，将图粗化为高级子结构。如下图所示，这种架构设计可用于提取图的各级表示和执行图分类任务。

![img](https://picx.zhimg.com/v2-0e2dfabc74cdae64f5d695caa2cbb4cf_r.jpg)

###### Spectral-based GCN：

**预备知识与相关公式**：

无向图的鲁棒数学表示是正则化图拉普拉斯矩阵：

$L = I_n - D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$，

其中A为图的邻接矩阵，D为对角矩阵且

$D_{ii} = \sum_{j}A(i,j)$​

正则化图拉普拉斯矩阵具有实对称半正定的性质。利用这个性质，正则化拉普拉斯矩阵可以分解为

$L = U∧U^T$

其中∧表示对角元素为L的特征值的对角矩阵，$U = [u_0,u_1,...,u_n-1] \in R^{N×N}$，U是由L的特征向量构成的矩阵。正则化拉普拉斯矩阵的特征向量构成了一组正交基。

在图信号处理过程中，一个图的信号$X \in R^N$是一个由图的各个节点组成的特征向量，$x_i$代表第i个节点。

对图X的傅里叶变换由此被定义为：

$F(x) = U^Tx$

傅里叶反变换则为：

$F^{-1}(\overline{x}) = U\overline{x}$​

其中$\overline{x}$为傅里叶变换后的结果。

傅里叶变换将输入图信号投影到正交空间，而这个正交空间的基(base)则是由正则化图拉普拉斯的特征向量构成。

转换后得到的信号$\overline{x}$的元素是新空间中图信号的坐标，因此原来的输入信号可以表示为

$x = \sum_{i}\overline{X_{i}}u_i$

以上式子是傅里叶反变换的结果。

现在定义对输入信号x的图卷积操作：

$x*Gg = F^{-1}(F(x)⊙F(g)) = U(U^Tx⊙U^Tg)$

其中$g \in R^N$​是我们定义的滤波器；其中⊙表示哈达玛积（Hadamard product）

 Hadamard乘积 是矩阵的一类运算，对形状相同的矩阵进行运算，并产生相同维度的第三个矩阵。在数学中，Hadamard乘积（也称为 Schur乘积或逐元素乘积）是一种二元运算，它用两个具有相同维数的矩阵产生另一个具有相同维数的矩阵，其中每个元素 ( i , j ) 是原始两个矩阵的元素 ( i , j ) 的乘积。

假如我们定义这样一个滤波器：

$g_\theta = diag(U^Tg)$

我们的图卷积操作可以简化表示为：
$x*Gg_\theta = Ug_{\theta}U^Tx$



###### Spatial-based GCN：

学习参考资料：

[(2 条消息) 图神经网络与深度学习在智能交通中的应用：综述Survey - 知乎](https://zhuanlan.zhihu.com/p/143605050)

图注意力网络（Graph Attention Networks）：

图自编码器（Graph Autoencoders）：

图生成网络（Graph Generative Networks）：

图时空网络（Graph Spatial-temporal Networks）：

